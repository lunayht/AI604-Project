default = {
    dim = 768
    ff_dim = 3072
    num_heads = 12
    num_layers = 12
    attention_dropout_rate= 0.0
    dropout_rate = 0.1
    representation_size = 768
    classifier = 'token'
}

b16 = ${default} {
    patches = [16, 16]
}

b32 = ${default} {
    patches = [32, 32]
}

l16 = ${default} {
    patches = [16, 16]
    dim = 1024
    ff_dim = 4096
    num_heads = 16,
    num_layers = 24,
    attention_dropout_rate = 0.0
    dropout_rate = 0.1
    representation_size = 1024
}

l32 = ${l16} {
    patches = [32, 32]
}